import streamlit as st
import tiktoken
from loguru import logger

from langchain.chains import ConversationalRetrievalChain
from langchain.chat_models import ChatOpenAI

from langchain.document_loaders import PyPDFLoader
from langchain.document_loaders import Docx2txtLoader
from langchain.document_loaders import UnstructuredPowerPointLoader

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings

from langchain.memory import ConversationBufferMemory
from langchain.vectorstores import FAISS

# from streamlit_chat import message
from langchain.callbacks import get_openai_callback
from langchain.memory import StreamlitChatMessageHistory

def main():
    # st.set_page_config(
    #     page_title="Library and Information Science OpenChat",
    #     page_icon=":books:"
    # )

    st.title("ğŸ“š LISBOTì—ê²Œ ë¬¼ì–´ë³´ì„¸ìš” â•")
    st.caption("ğŸ“¢ *Welcome to the Library and Information Science Q&A chat. This app is developed for the Introduction to Data Science course project for Spring 2024. Feel free to ask any questions to LISBOT. Whether you're looking for research help, resource recommendations, or answers to specific questions, LISBOT is here to assist you.*")
    st.caption("âœ”ï¸ **ë°˜ë“œì‹œ íŒŒì¼ì„ ë¨¼ì € ì²¨ë¶€í•œ ë’¤ OPENAPI KEYë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”** ğŸ–‹ï¸")
    st.caption("âœ”ï¸ **LISBOTì€ ì²¨ë¶€í•œ ìë£Œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤. ìë£Œë¥¼ ì—…ë¡œë“œí•˜ê³  ê¶ê¸ˆí•œ ì ì„ ë¬¼ì–´ë³´ì„¸ìš”** ğŸ’¬")

    if "conversation" not in st.session_state:
        st.session_state.conversation = None

    if "chat_history" not in st.session_state:
        st.session_state.chat_history = None

    if "processComplete" not in st.session_state:
        st.session_state.processComplete = None

    with st.sidebar:
        uploaded_files = st.file_uploader("ì—¬ê¸°ì— íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.", type=['pdf', 'docx'], accept_multiple_files=True)
        openai_api_key = st.text_input("OpenAI API Key", key="chatbot_api_key", type="password")
        process = st.button("Process")

    if process:
        if not uploaded_files:
            st.info("ë¨¼ì € íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")
            st.stop()
        if not openai_api_key:
            st.info("OpenAI API keyë¥¼ ë‹¤ì‹œ ì…ë ¥í•˜ì„¸ìš”.")
            st.stop()

        files_text = get_text(uploaded_files)
        text_chunks = get_text_chunks(files_text)
        vetorestore = get_vectorstore(text_chunks)

        st.session_state.conversation = get_conversation_chain(vetorestore, openai_api_key)
        st.session_state.processComplete = True

    if 'messages' not in st.session_state:
        st.session_state['messages'] = [{"role": "assistant",
                                         "content": "ì•ˆë…•í•˜ì„¸ìš” ì €ëŠ” ai ì‚¬ì„œ LISBOTì…ë‹ˆë‹¤. ê¶ê¸ˆí•œ ê²ƒì´ ìˆë‚˜ìš”?"}]

    # CSS for chat messages
    st.markdown("""
        <style>
            .chat-container {
                display: flex;
                flex-direction: column;
                max-width: 600px;
                margin: 0 auto;
            }
            .chat-message {
                display: flex;
                align-items: flex-start;
                margin: 10px 0;
                padding: 10px;
                border-radius: 10px;
                max-width: 80%;
            }
            .chat-message.user {
                background-color: #e6ffe6;
                align-self: flex-end;
            }
            .chat-message.assistant {
                background-color: #f0f2f6;
                align-self: flex-start;
            }
            .chat-message .avatar {
                width: 40px;
                height: 40px;
                margin-right: 10px;
            }
            .chat-input-container {
                display: flex;
                align-items: center;
                justify-content: center;
                padding: 10px;
                background-color: #f0f2f6;
                border-top: 1px solid #ccc;
            }
            .chat-input {
                width: 100%;
                padding: 10px;
                border-radius: 20px;
                border: 1px solid #ccc;
                margin-right: 10px;
                outline: none;
            }
            .chat-button {
                padding: 10px 20px;
                border: none;
                border-radius: 20px;
                background-color: #4CAF50;
                color: white;
                cursor: pointer;
            }
        </style>
    """, unsafe_allow_html=True)

    st.markdown("<div class='chat-container'>", unsafe_allow_html=True)
    for message in st.session_state.messages:
        if message["role"] == "assistant":
            st.markdown(f"""
                <div class='chat-message assistant'>
                    <span class='avatar'>ğŸ¤–</span>
                    <span>{message['content']}</span>
                </div>
                """, unsafe_allow_html=True)
        else:
            st.markdown(f"""
                <div class='chat-message user'>
                    <span class='avatar'>ğŸ§‘</span>
                    <span>{message['content']}</span>
                </div>
                """, unsafe_allow_html=True)
    st.markdown("</div>", unsafe_allow_html=True)

    # Chat input
    query = st.text_input("ì—¬ê¸°ì— ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”.", key="chat_input")
    if st.button("Send", key="send_button"):
        if query:
            st.session_state.messages.append({"role": "user", "content": query})
            st.markdown(f"""
                <div class='chat-message user'>
                    <span class='avatar'>ğŸ§‘</span>
                    <span>{query}</span>
                </div>
                """, unsafe_allow_html=True)

            chain = st.session_state.conversation

            with st.spinner("Thinking..."):
                result = chain({"question": query})
                with get_openai_callback() as cb:
                    st.session_state.chat_history = result['chat_history']
                response = result['answer']
                source_documents = result['source_documents']

                st.markdown(f"""
                    <div class='chat-message assistant'>
                        <span class='avatar'>ğŸ¤–</span>
                        <span>{response}</span>
                    </div>
                    """, unsafe_allow_html=True)
                with st.expander("ì°¸ê³  ë¬¸ì„œ í™•ì¸"):
                    st.markdown(source_documents[0].metadata['source'], help=source_documents[0].page_content)
                    st.markdown(source_documents[1].metadata['source'], help=source_documents[1].page_content)
                    st.markdown(source_documents[2].metadata['source'], help=source_documents[2].page_content)

            # Add assistant message to chat history
            st.session_state.messages.append({"role": "assistant", "content": response})

def tiktoken_len(text):
    tokenizer = tiktoken.get_encoding("cl100k_base")
    tokens = tokenizer.encode(text)
    return len(tokens)

def get_text(docs):
    doc_list = []

    for doc in docs:
        file_name = doc.name  # doc ê°ì²´ì˜ ì´ë¦„ì„ íŒŒì¼ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©
        with open(file_name, "wb") as file:  # íŒŒì¼ì„ doc.nameìœ¼ë¡œ ì €ì¥
            file.write(doc.getvalue())
            logger.info(f"Uploaded {file_name}")
        if '.pdf' in doc.name:
            loader = PyPDFLoader(file_name)
            documents = loader.load_and_split()
        elif '.docx' in doc.name:
            loader = Docx2txtLoader(file_name)
            documents = loader.load_and_split()
        elif '.pptx' in doc.name:
            loader = UnstructuredPowerPointLoader(file_name)
            documents = loader.load_and_split()

        doc_list.extend(documents)
    return doc_list

def get_text_chunks(text):
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=900,
        chunk_overlap=100,
        length_function=tiktoken_len
    )
    chunks = text_splitter.split_documents(text)
    return chunks

def get_vectorstore(text_chunks):
    embeddings = HuggingFaceEmbeddings(
        model_name="jhgan/ko-sroberta-multitask",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': True}
    )
    vectordb = FAISS.from_documents(text_chunks, embeddings)
    return vectordb

def get_conversation_chain(vetorestore, openai_api_key):
    llm = ChatOpenAI(openai_api_key=openai_api_key, model_name='gpt-4', temperature=0)
    conversation_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        chain_type="stuff",
        retriever=vetorestore.as_retriever(search_type='mmr', verbose=True),
        memory=ConversationBufferMemory(memory_key='chat_history', return_messages=True, output_key='answer'),
        get_chat_history=lambda h: h,
        return_source_documents=True,
        verbose=True
    )
    return

